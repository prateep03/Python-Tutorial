{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff\n",
    "\n",
    "Whenever we discuss model prediction, it's important to understand prediction errors. There is a tradeoff between a model's ability to minimize *training* and *generalization*(or test) errors. \n",
    "\n",
    "## What is bias ?\n",
    "\n",
    "Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. A model with very high bias pays little attention to the training data and overtly simplifies the model. Other name for **high bias** is **underfitting**, high error on training and test data.\n",
    "\n",
    "## What is variance ?\n",
    "\n",
    "Variance is the variability of model prediction for a given data point. A model with high variance pays lot of attention to training data and does not generalize on unseen data. As a result, such a model performs very well on training data but has high error on unseen data. Other name for **low bias, high variance** is **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A bit of mathematics\n",
    "\n",
    "--------------\n",
    "--------------\n",
    "**Note:** The below proof is optional. You can go over it to understand the concept behind bias/variance decomposition. However, it's not important for our future lectures.\n",
    "--------------\n",
    "--------------\n",
    "\n",
    "Assume, we are given ad dataset $D = \\left\\{(\\mathbf{x}_1,y_1), \\cdots (\\mathbf{x}_n,y_n) \\right\\}$, drawn *independently and identically* from some distribution $\\mathrm{P}$, i.e. $(\\mathbf{x},y) \\sim \\mathrm{P}$. We will look at the generalization, otherwise called test error, of a classifier. What is the expected label for a point, given the dataset $D$ ? Mathematically, this is the *expectation*, or $\\mathbb{E}$, of a data point $\\mathbf{x} \\in \\Re^{d}$ given the distribution $\\mathrm{P}$. We can write this as:\n",
    "\n",
    "<font size=\"4\">\n",
    "$$\n",
    "\\text{Expected label: } \\: \\: \\bar{y}(\\mathbf{x}) = \\underset{y|\\mathbf{x}}{\\mathbb{E}} \\;[y] = \\underset{y}\\int y P(y|\\mathbf{x}) \\partial y\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A$ be a your favorite machine learning algorithm which you apply on the dataset $D$. The algorithm produces a model, call it $h_D = A(D)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the *expected test error* on unseen data points $\\mathbf{x}$ with labels $y$, given a model $h_D$ and a dataset $D$, is computed as:\n",
    "\n",
    "<font size=\"4\">\n",
    "$$\n",
    "\\text{Expected test error: } \\bar{\\epsilon} = \\underset{(\\mathbf{x},y) \\sim P}{\\mathbb{E}} \\left [ \\left ( h_D(\\mathbf{x}) - y\\right)^2 \\right ] = \\underset{x}\\int \\underset{y}\\int \\left ( h_D(\\mathbf{x}) - y\\right)^2 P(\\mathbf{x},y) \\partial \\mathbf{x} \\partial y\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above error is specific to the dataset $D$. However, remember that $D$ itself is drawn from the distribution $\\mathrm{P}$, i.e. $D \\sim \\mathrm{P}$. Therefore, $D$ and heceforth the model $h_D$ are random variables as well. Hence, the *expected model* ($\\bar{h}$) given an algorithm $A$, can be similarly computed as:\n",
    "\n",
    "<font size=\"4\">\n",
    "$$\n",
    "\\text{Expected model: } \\bar{h} = \\underset{D \\sim P}{\\mathbb{E}} [h_D] = \\underset{D}\\int h_D P(D) \\partial D\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the last two terms, we can rewrite the *expected genralized test error* for algorithm $A$, as follows:\n",
    "\n",
    "<font size=\"4\">\n",
    "$$\n",
    "\\text{Expected generalized test error: } \\bar{\\epsilon_{g}} = \\underset{(\\mathbf{x},y) \\sim P \\\\ D \\sim P}{\\mathbb{E}} \\left [ \\left ( h_D(\\mathbf{x}) - y\\right)^2 \\right ] = \\underset{D}\\int \\underset{(\\mathbf{x},y)}\\int \\left ( h_D(\\mathbf{x}) - y\\right)^2 P(\\mathbf{x},y) P(D) \\partial D \\partial x \\partial y\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition of Generalized Test Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the *decomposition* of test error($\\bar{\\epsilon}$) into constituent terms.\n",
    "\n",
    "<font size=\"4\">\n",
    "$$\n",
    "(1) \\; \\; \n",
    "\\begin{align}\n",
    "\\underset{\\mathbf{x},y,D}{\\mathbb{E}} \\left [ \\left ( h_D(\\mathbf{x}) - y\\right)^2 \\right ] &= \\underset{\\mathbf{x},y,D}{\\mathbb{E}} \\left [ \\left [ \\left ( h_D(\\mathbf{x}) - \\bar{h}(\\mathbf{x}) \\right) + \\left ( \\bar{h}(\\mathbf{x}) - y\\right) \\right ]^2 \\right ] \\\\\n",
    "&= \\underset{\\mathbf{x},D}{\\mathbb{E}}  \\left [ \\left ( h_D(\\mathbf{x}) - \\bar{h}(\\mathbf{x})\\right)^2 \\right ] + 2 \\underset{\\mathbf{x},y,D}{\\mathbb{E}}  \\left [ \\left ( h_D(\\mathbf{x}) - \\bar{h}(\\mathbf{x})\\right) \\left(\\bar{h}(\\mathbf{x}) - y\\right) \\right ] +  \\underset{\\mathbf{x},y}{\\mathbb{E}}  \\left [\\bar{h}(\\mathbf{x}) - y)^2 \\right ]\n",
    "\\end{align}\n",
    "$$\n",
    "</font>\n",
    "\n",
    "The middle term in the above equation evaluates to 0, as follows:\n",
    "\n",
    "<font size=\"4\">\n",
    "$$\n",
    "\\begin{align}\n",
    "\\underset{\\mathbf{x},y,D}{\\mathbb{E}}  \\left [ \\left ( h_D(\\mathbf{x}) - \\bar{h}(\\mathbf{x})\\right) \\left(\\bar{h}(\\mathbf{x}) - y\\right) \\right ] &= \\underset{x,y}{\\mathbb{E}} \\left [ \\underset{D}{\\mathbb{E}} \\left(  h_D(\\mathbf{x}) - \\bar{h}(\\mathbf{x}) \\right) \\left( \\bar{h}(\\mathbf{x}) - y\\right) \\right ] \\\\\n",
    "&= \\underset{x,y}{\\mathbb{E}} \\left[\\left( \\underset{D}{\\mathbb{E}} \\left[ h_D(\\mathbf{x}) \\right] - \\bar{h}(\\mathbf{x}) \\right) \\left( \\bar{h}(\\mathbf{x}) - y\\right) \\right] \\\\\n",
    "&= \\underset{x,y}{\\mathbb{E}} \\left[\\left( \\bar{h}(\\mathbf{x}) - \\bar{h}(\\mathbf{x}) \\right) \\left( \\bar{h}(\\mathbf{x}) - y\\right) \\right] \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "</font>\n",
    "\n",
    "Let us go back to (1) now.\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{x},y,D}{\\mathbb{E}} \\left [ \\left ( h_D(\\mathbf{x}) - y\\right)^2 \\right ] = \\underset{\\mathbf{x},D}{\\mathbb{E}}  \\left [ \\left ( h_D(\\mathbf{x}) - \\bar{h}(\\mathbf{x})\\right)^2 \\right ] + \\underset{\\mathbf{x},y}{\\mathbb{E}}  \\left [\\bar{h}(\\mathbf{x}) - y)^2 \\right ]\n",
    "$$\n",
    "\n",
    "The first term is the **variance** of your model $h_D$ (remember the definition of variance). Lets look at the second term now.\n",
    "\n",
    "$$\n",
    "(2) \\; \\;\n",
    "\\begin{align}\n",
    "\\underset{\\mathbf{x},y}{\\mathbb{E}} \\left[ \\left( \\bar{h}(\\mathbf{x}) - y \\right)^2 \\right] &= \\underset{\\mathbf{x},y}{\\mathbb{E}} \\left[ \\left[ \\left( \\bar{h}(\\mathbf{x}) - \\bar{y}(\\mathbf{x})\\right) + \\left( \\bar{y}(\\mathbf{x}) - y \\right) \\right]^2 \\right]\\\\\n",
    "&= \\underset{\\mathbf{x},y}{\\mathbb{E}} \\left[ \\left(\\bar{h}(x) - \\bar{y}(x)\\right)^2 \\right] + \\underset{x}{\\mathbb{E}} \\left[ \\left(\\bar{y}(x) - y\\right)^2 \\right] + 2 \\underset{\\mathbf{x},y}{\\mathbb{E}} \\left[\\left( \\bar{h}(x) - \\bar{y}(x) \\right) \\left( \\bar{y}(x) - y \\right) \\right]\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third term, as before, evaluates to 0, as follows:\n",
    "\n",
    "<font size=\"4\">\n",
    "$$\n",
    "\\begin{align}\n",
    "\\underset{\\mathbf{x},y}{\\mathbb{E}} \\left[\\left( \\bar{h}(x) - \\bar{y}(x) \\right) \\left( \\bar{y}(x) - y \\right) \\right] &= \\underset{\\mathbf{x}}{\\mathbb{E}} \\left[ \\underset{y|x}{\\mathbb{E}} \\left[ \\bar{y}(\\mathbf{x}) - y\\right] \\left( \\bar{h}(x) - \\bar{y}(x)\\right) \\right] \\\\\n",
    "&= \\underset{\\mathbf{x}}{\\mathbb{E}} \\left[ \\left(\\bar{y}(\\mathbf{x}) - \\underset{y|x}{\\mathbb{E}}(y)\\right) \\left( \\bar{h}(x) - \\bar{y}(x)\\right) \\right] \\\\\n",
    "&= \\underset{\\mathbf{x}}{\\mathbb{E}} \\left[ \\left(\\bar{y}(\\mathbf{x}) - \\bar{y}(\\mathbf{x})\\right) \\left( \\bar{h}(x) - \\bar{y}(x)\\right) \\right] \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$\n",
    "</font>\n",
    "\n",
    "Looking back at (2), we now get.\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{x},y}{\\mathbb{E}} \\left[ \\left( \\bar{h}(\\mathbf{x}) - y \\right)^2 \\right] = \\underset{\\mathbf{x},y}{\\mathbb{E}} \\left[ \\left(\\bar{h}(x) - \\bar{y}(x)\\right)^2 \\right] + \\underset{x}{\\mathbb{E}} \\left[ \\left(\\bar{y}(x) - y\\right)^2 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term is the **bias**^2, which is the error that we get from the classifier when training on many samples of data distribution $P$. It comes from the assumptions the algorithm makes about the data. \n",
    "\n",
    "The second term is the **noise**, which is data-dependent. This difference is insurmountable in data science.\n",
    "\n",
    "Putting all the pieces back together, we get the final decomposition.\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{x},y,D}{\\mathbb{E}} \\left [ \\left ( h_D(\\mathbf{x}) - y\\right)^2 \\right ] = \\underset{\\text{variance}}{\\underset{\\mathbf{x},D}{\\mathbb{E}}  \\left [ \\left ( h_D(\\mathbf{x}) - \\bar{h}(\\mathbf{x})\\right)^2 \\right ]} + \\underset{\\text{bias}^2}{\\underset{\\mathbf{x},y}{\\mathbb{E}} \\left[ \\left(\\bar{h}(x) - \\bar{y}(x)\\right)^2 \\right]} + \\underset{noise}{\\underset{x}{\\mathbb{E}} \\left[ \\left(\\bar{y}(x) - y\\right)^2 \\right]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of equations above. But, hopefully, the concept behind this decomposition is clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"..\\data\\bv-bullseye.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above picture([Source](http://scott.fortmann-roe.com/docs/BiasVariance.html)) intuitively visualizes bias and variance using a bulls-eye diagram. Imagine the centre of the target is a model that *perfectly* predicts the correct values. As we move away from the bulls-eye, our predictions gets worse. If we could repeat our model building process to get a number of separate hits on the target, each hit representing an individual model($h_D$), given a training dataset $D$. Sometimes we will get a good distribution of training data so we predict well, whereas sometimes the data will be full of outliers, in which case our predictions degrade. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can now make the classical *bias-variance tradeoff* plot as shown below. You should try to aim for the optimal balance between bias and variance, as shown below. This typically means that your training and test errors should be approximately the same.\n",
    "\n",
    "<img src=\"../data/bias-variance.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A practical example for showing bias-variance tradeoff in *linear regression* is [here](https://gist.github.com/fabgoos/6788818)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
